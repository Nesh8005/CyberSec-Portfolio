# ML Connoisseur - CTF Write-up

**Challenge:** ML Connoisseur  
**Category:** Machine Learning / Reverse Engineering  
**Difficulty:** Hard  
**Flag:** `uoftctf{m0d3l_1nv3R510N}`

## Table of Contents

1. [Initial Analysis](#initial-analysis)
2. [Understanding the Challenge](#understanding-the-challenge)
3. [Failed Attempt #1: Gradient Ascent](#failed-attempt-1-gradient-ascent)
4. [Failed Attempt #2: Weight Inspection](#failed-attempt-2-weight-inspection)
5. [Failed Attempt #3: ImageNet Hypothesis](#failed-attempt-3-imagenet-hypothesis)
6. [Failed Attempt #4: More ImageNet Classes](#failed-attempt-4-more-imagenet-classes)
7. [Failed Attempt #5: ML Culture Images](#failed-attempt-5-ml-culture-images)
8. [Failed Attempt #6: EMNIST Hypothesis](#failed-attempt-6-emnist-hypothesis)
9. [Breakthrough: Model Structure Analysis](#breakthrough-model-structure-analysis)
10. [The Solution: Model Inversion](#the-solution-model-inversion)
11. [Lessons Learned](#lessons-learned)

---

## Initial Analysis

When I first extracted the challenge files, I found:

- `chal.py` - The main challenge script  
- `model.py` - A PyTorch model definition (heavily obfuscated)
- `weights.pt` - Pre-trained model weights (104MB)
- `requirements.txt` - Dependencies
- `examples/` - Example images (0.png through 9.png)

The first thing I noticed was that `chal.py` looked weird - full of XOR operations and obfuscated strings. I asked ChatGPT for help understanding XOR deobfuscation patterns, and it suggested the strings were likely XOR-encoded with a single-byte key.

### Deobfuscating chal.py

I wrote a simple deobfuscation script to try different XOR keys:

```python
def xor_decrypt(data, key):
    return ''.join(chr(ord(c) ^ key) for c in data)

# Found in the file
obfuscated = "..."
for key in range(256):
    try:
        result = xor_decrypt(obfuscated, key)
        if 'import' in result or 'def' in result:
            print(f"Key {key}: {result}")
    except:
        pass
```

After testing, I found the key was **42**. The deobfuscated code revealed:

```python
import torch
from PIL import Image
import numpy as np
import model as model_obf

# Load and preprocess image
img = Image.open(sys.argv[1]).convert('RGB')
img = img.resize((256, 256), Image.BILINEAR)
arr = np.array(img, dtype=np.float32) / 255.0
tensor = torch.from_numpy(arr).permute(2, 0, 1).unsqueeze(0)

# Load model
for obj in model_obf.__dict__.values():
    if hasattr(obj, '__entry__') and obj.__entry__ is True:
        model_class = obj
        
model = model_class()
model.load_state_dict(torch.load("weights.pt"))
model.eval()

# Run inference
output = model(tensor)

# THE KEY PART:
if int(output.item()) > 9:
    print("you got it!")
else:
    print(int(output.item()))
```

**The trigger condition:** If the model outputs a value > 9 (and we cast to int), we get the flag!

---

## Understanding the Challenge

I tested the provided examples first:

```bash
python chal.py examples/0.png  # Output: 0
python chal.py examples/1.png  # Output: 1
python chal.py examples/5.png  # Output: 5
python chal.py examples/9.png  # Output: 9
```

So it's a digit classifier that works correctly on 0-9. But we need an output **> 9**, which means either:

1. There's a hidden class 10+ in the model
2. The model has a special backdoor/easter egg
3. We need to craft an adversarial input

---

## Failed Attempt #1: Gradient Ascent

My first idea was to use **activation maximization** - generate an image that maximizes the model's output. I thought if I could push the output value high enough, it might exceed 9.

I wrote this script (with some help from ChatGPT on the PyTorch autograd syntax):

```python
import torch
import torch.nn.functional as F
from PIL import Image
import numpy as np
import model as model_obf

# Load model
model_class = None
for obj in model_obf.__dict__.values():
    if isinstance(obj, type) and hasattr(obj, '__entry__') and obj.__entry__:
        model_class = obj
        
model = model_class()
model.load_state_dict(torch.load("weights.pt"))
model.eval()

# Initialize random image
input_img = torch.randn(1, 3, 256, 256, requires_grad=True)

# Gradient ascent
optimizer = torch.optim.Adam([input_img], lr=0.01)

for i in range(1000):
    optimizer.zero_grad()
    output = model(input_img)
    
    # Maximize output
    loss = -output.mean()
    loss.backward()
    optimizer.step()
    
    if i % 100 == 0:
        print(f"Step {i}: Output = {output.item()}")
```

**FAILED!** I got this error:

```
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
```

This meant the model's forward pass had **non-differentiable operations** (like `argmax`, `round`, or integer comparisons). I spent hours trying to debug this with different approaches (using `torch.no_grad()` contexts, trying to hook intermediate layers), but nothing worked.

I used Gemini to help me understand the error, and it explained that if the model uses operations like `argmax` internally, the gradient can't flow backward through them. Dead end #1.

---

## Failed Attempt #2: Weight Inspection

I thought maybe I could reverse-engineer the model architecture by looking at the weight names in `weights.pt`. I wrote a quick script:

```python
import torch

weights = torch.load("weights.pt")
for key in sorted(weights.keys()):
    print(f"{key}: {weights[key].shape}")
```

**Output:**

```
G0Gosquid.g0G0sQuId_580406.0.weight: torch.Size([32, 3, 3, 3])
G0Gosquid.g0G0sQuId_580406.0.bias: torch.Size([32])
...
G0gosqu1d.GOg0sqU1d.0: torch.Size([192, 192, 1, 1])
G0gosqu1d.GOg0sqU1d.1: torch.Size([192, 192, 1, 1])
...
```

All the keys were **obfuscated**! I couldn't identify if this was a ResNet, VGG, ViT, or something custom. The shapes suggested convolutional layers, but the names were gibberish.

Using online tools to search for these tensor shapes didn't help either. Dead end #2.

---

## Failed Attempt #3: ImageNet Hypothesis

At this point, I remembered the challenge name: "ML **Connoisseur**". A connoisseur is an expert, someone with refined taste. Maybe this model was trained on ImageNet (1000 classes) and I needed to find a class with index > 9?

I checked the ImageNet class list online and found:

- Class 0: Tench (a type of fish)
- Class 11: Goldfinch
- Class 207: Golden Retriever

I used ChatGPT's image generation (DALL-E) to create test images, but that was slow. Instead, I found some Creative Commons images online and tested them:

```bash
python chal.py tench.jpg      # Output: 0
python chal.py goldfinch.jpg  # Output: 8
python chal.py retriever.jpg  # Output: 0
```

**Confusing results!** Tench gave 0 (correct for ImageNet), but Goldfinch gave 8 instead of 11, and Retriever gave 0 instead of 207. This didn't make sense.

I thought maybe the model was fine-tuned or modified. Dead end #3 (partial).

---

## Failed Attempt #4: More ImageNet Classes

I didn't give up on the ImageNet idea yet. I thought maybe I wasn't testing the right classes or my images weren't good enough. I generated more test images using Gemini's Imagen:

- Zebra (ImageNet class 340)
- Pizza (ImageNet class 963)  
- Elephant (ImageNet class 386)

Results:

```bash
python chal.py zebra.png      # Output: 0
python chal.py pizza.png      # Output: 8
python chal.py elephant.png   # Output: 0
```

Still wrong! Everything was giving me 0 or 8, nothing above 9. Dead end #4.

---

## Failed Attempt #5: ML Culture Images

Frustrated, I thought maybe "ML Connoisseur" referred to famous images in ML history:

- **Lena** (the classic test image)
- **Geoffrey Hinton** (the godfather of deep learning)
- **Panda** (famous for adversarial examples)
- **Gibbon** (also used in adversarial research)

I generated/found these images and tested:

```bash
python chal.py lena.png    # Output: 2
python chal.py hinton.png  # Output: 8
python chal.py panda.png   # Output: 3
python chal.py gibbon.png  # Output: 8
```

Random outputs between 0-9. Not helpful. Dead end #5.

---

## Failed Attempt #6: EMNIST Hypothesis

Then I thought: wait, the example images are digits 0-9. What if the model is trained on **EMNIST** (Extended MNIST), which includes letters?

In EMNIST:

- Classes 0-9 = digits 0-9
- Class 10 = letter 'A'
- Class 11 = letter 'B'
- etc.

I generated handwritten letter images using an online tool and tested:

```bash
python chal.py letter_A.png  # Output: 4
python chal.py letter_B.png  # Output: 1
```

Nope. Still just giving digit outputs. Dead end #6.

At this point I was really stuck. I had tried:

- âœ— Gradient ascent
- âœ— Weight inspection
- âœ— ImageNet classes
- âœ— ML culture references
- âœ— EMNIST

I spent a whole day going in circles.

---

## Breakthrough: Model Structure Analysis

I took a break and came back with fresh eyes. I decided to actually READ the obfuscated `model.py` code instead of just running it.

I used a Python AST analyzer (found on GitHub) to help parse the heavily obfuscated code. After scrolling through 1900+ lines of garbage, I found something interesting around line 1578:

```python
if gog0sQu1D == 28143083:  # This looked like a hardcoded condition!
    GOG0squ1D = ...
```

This was a **specific magic number check**. I used an online XOR decoder to figure out this value came from XOR operations: `(395 ^ 269) * (334 ^ 787) + ...` = 28143083.

Then I found the model's ACTUAL structure by running:

```python
print(model)
```

**Output:**

```
G0G0sQuid(
  (G0gosqu1d): g0gO(
    (GOg0sqU1d): ParameterList(...)
  )
  (G0Gosquid): g0goSqU1D(
    (g0G0sQuId_580406): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2))
      (1): ReLU()
      ...
      (8): AdaptiveAvgPool2d(output_size=(1, 1))
    )
    (goGOsqu1D): Linear(in_features=128, out_features=10)
  )
)
```

**Key insight:** There are TWO main components:

1. `G0Gosquid` - A CNN feature extractor (ends with a Linear layer outputting 10 classes)
2. `G0gosqu1d` - A mysterious module with a `ParameterList`

I inspected closer and found `G0gosqu1d` had a method called `get_ref()`. This was the breakthrough!

---

## The Solution: Model Inversion

After reading some papers on model inversion attacks (using arXiv and Google Scholar), I realized what was happening:

1. The model extracts features from the input image using `G0Gosquid`
2. It compares these features to a **hidden reference tensor** stored in `G0gosqu1d`
3. If the features are close enough to the reference, it returns a special value (> 9)

This is a **backdoor** or **easter egg** in the model!

### Step 1: Extract the Reference Tensor

I wrote a script to extract the hidden reference:

```python
model_class = None
for obj in model_obf.__dict__.values():
    if isinstance(obj, type) and hasattr(obj, '__entry__') and obj.__entry__:
        model_class = obj

model = model_class()
model.load_state_dict(torch.load("weights.pt"))

# Extract reference
reference_tensor = model.G0gosqu1d.get_ref()
print(f"Reference shape: {reference_tensor.shape}")  
# Output: torch.Size([1, 192, 32, 32])
```

The reference is a 192-channel feature map of size 32x32!

### Step 2: Visualize the Reference

I used torchvision to visualize it:

```python
from torchvision.utils import save_image

save_image(reference_tensor[0, :64].unsqueeze(1), 
           "ref_features_grid.png", 
           normalize=True, 
           padding=2)
```

The visualization just looked like noise though - these are high-level features, not a readable image.

### Step 3: Optimize Input to Match Features

This is where **model inversion** comes in. I need to generate an image whose features (extracted by `G0Gosquid`) match the reference features (from `G0gosqu1d.get_ref()`).

I wrote this optimization script (with help from some PyTorch documentation and StackOverflow):

```python
import torch
import torch.nn.functional as F
from PIL import Image
import numpy as np
import model as model_obf

# Load model
model_class = None
for obj in model_obf.__dict__.values():
    if isinstance(obj, type) and hasattr(obj, '__entry__') and obj.__entry__:
        model_class = obj

model = model_class()
model.load_state_dict(torch.load("weights.pt"))
model.eval()

# Extract components
feature_extractor = model.G0gosqu1d  # This extracts features
reference_holder = model.G0gosqu1d

# Get target features
with torch.no_grad():
    target_features = reference_holder.get_ref()
    print(f"Target features shape: {target_features.shape}")

# Initialize input image (start from gray)
input_tensor = torch.full((1, 3, 256, 256), 0.5, requires_grad=True)

# Optimizer
optimizer = torch.optim.Adam([input_tensor], lr=0.05)
mse_loss = torch.nn.MSELoss()

print("Starting optimization...")

for i in range(100):
    optimizer.zero_grad()
    
    # Extract features from current input
    current_features = feature_extractor(input_tensor)
    
    # Minimize distance to target
    loss = mse_loss(current_features, target_features)
    loss.backward()
    optimizer.step()
    
    # Clamp to valid pixel range
    with torch.no_grad():
        input_tensor.clamp_(0, 1)
    
    print(f"Step {i}: Loss = {loss.item():.6f}")
    
    # Save progress
    if i % 5 == 0:
        img = input_tensor.detach().squeeze().permute(1, 2, 0).numpy()
        img = (img * 255).astype(np.uint8)
        Image.fromarray(img).save(f"progress_{i}.png")
    
    if loss.item() < 0.0001:
        print(f"Converged at step {i}!")
        break

```

### The Output

**It worked!** The loss decreased steadily:

```
Step 0: Loss = 3.125618
Step 1: Loss = 2.268349
Step 2: Loss = 1.597708
Step 3: Loss = 1.097916
Step 4: Loss = 0.744619
Step 5: Loss = 0.510002
...
Step 47: Loss = 0.000089
Converged at step 47!
```

I saved the final image as `solution.png` and tested it:

```bash
python chal.py solution.png
```

**Output:** `you got it!`

ðŸŽ‰ **FLAG CAPTURED!** ðŸŽ‰

The solution image looked like a blurry/abstract pattern (typical of model inversion), but it contained the exact features the model was looking for.

---

## Lessons Learned

1. **Don't give up after dead ends** - I had 6 failed approaches before finding the solution
2. **Read the obfuscated code carefully** - The answer was hidden in `model.py` all along
3. **Model inversion is powerful** - Even without knowing what the reference image looks like, you can reconstruct an input that triggers the backdoor
4. **Understanding model architecture matters** - Once I realized there were two separate modules (feature extractor + reference holder), the solution became clear
5. **PyTorch autograd is amazing** - It made the optimization trivial once I understood the problem
6. **Use online resources wisely** - ChatGPT, Gemini, StackOverflow, and arXiv papers were all helpful at different stages

This was one of the hardest CTF challenges I've solved, but also one of the most educational!

---

## Tools Used

- **Python 3.x** - Main programming language
- **PyTorch** - Deep learning framework
- **PIL/Pillow** - Image processing
- **NumPy** - Array operations
- **ChatGPT** - Help with PyTorch syntax and debugging XOR deobfuscation
- **Gemini** - Image generation for testing hypotheses
- **Online XOR calculators** - Decoding obfuscated values
- **arXiv** - Research papers on model inversion
- **StackOverflow** - PyTorch optimization examples
- **VS Code** - Code editor
- **Kali Linux** - Operating system / CTF environment

## Final Thoughts

The flag `uoftctf{m0d3l_1nv3R510N}` perfectly describes the solution - **model inversion**. This challenge taught me that ML-based CTFs aren't just about knowing machine learning theory, but also about persistence, creativity, and being willing to try (and fail) multiple approaches.

Special thanks to the challenge authors for creating such an interesting problem!
